{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #Haven't used\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv('https://raw.githubusercontent.com/MElHussieni/Twitter-US-Airline-Sentiment-Analysis/master/Tweets.csv')\n",
    "data1 = pd.read_csv('airline_tweets_nov_08-nov_15.csv')\n",
    "data2 = pd.read_csv('nlp_tweets_sample_11_16.csv')\n",
    "data3 = pd.read_csv('airline_tweets_nov_17-nov_24.csv')\n",
    "data4 = pd.read_csv('airline_tweets_nov_24-nov_27.csv')\n",
    "data5 = pd.read_csv(\"airline_tweets_nov_25-Dec_02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.rename(columns={'entities_hashtag':'hashtags',   # renaming columns from api search to match kaggle data\n",
    "                     'created_at': 'tweet_created',\n",
    "                     'coordinates':'tweet_coord',\n",
    "                     'full_text':'text',\n",
    "                     'user_name':'name',\n",
    "                     'id':'tweet_id'})  \n",
    "\n",
    "#sampling 10% of all tweets collected per week to keep dataset size maneagable \n",
    "\n",
    "data1=data1.sample(frac=0.1, replace=True)                 \n",
    "data2=data2.sample(frac=0.1, replace=True)\n",
    "data3=data3.sample(frac=0.1, replace=True)\n",
    "data4=data4.sample(frac=0.1, replace=True)\n",
    "data5=data5.sample(frac=0.1, replace=True)\n",
    "\n",
    "api_tweets = pd.concat([data1,data2,data3,data4,data5])        # merging API dataframes together together \n",
    "\n",
    "del api_tweets['user_screen_name']\n",
    "del api_tweets['Unnamed: 0']\n",
    "\n",
    "tweets=pd.concat([data,api_tweets])                     #merging kaggle and api tweets dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word_tokenize(tweet) for tweet in tweets[:]['text']]\n",
    "sents = [sent_tokenize(tweet) for tweet in tweets[:]['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "tt = TweetTokenizer() #preform better on tweets\n",
    "\n",
    "temp = tweets['text'].apply(tt.tokenize)\n",
    "\n",
    "temp2 = temp.apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "#need to be lowered to get rid of all the stopwords\n",
    "temp2 = temp2.apply(lambda x: [word for word in x if word not in stop]) \n",
    "temp2 = temp2.apply(lambda x: [word for word in x if word not in stop]) \n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "tweets['full_text_processed'] = temp2.apply(lambda x: [wnl.lemmatize(word) for word in x]) \n",
    "\n",
    "tweets['full_text_processed'] = tweets['full_text_processed'].apply(lambda x: ' ' .join([word for word in x]))\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_words=[word_tokenize(tweet) for tweet in tweets['full_text_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expletives_list = ['shit','fuck','damn','^hell(?!o)','crap','suck']\n",
    "upper_count=[]\n",
    "caps_ratio=[]\n",
    "num_expletives=[] \n",
    "pun_ratio=[]\n",
    "hashtags=[]\n",
    "\n",
    "regex=\"^#\"                                   # regular expression that matches hashtags\n",
    "\n",
    "for word_list in words:                                 # iterates over every tweet\n",
    "    hashtag='none'\n",
    "    uppers=0                                             # counters to keep track of \n",
    "    expletives=0\n",
    "    pun=0\n",
    "    i=0\n",
    "    for word in word_list:                              # iterates over every word in tweet\n",
    "        \n",
    "        if word.isupper():                               \n",
    "            uppers+=1                                     # adds 1 to count if uppercase word\n",
    "        \n",
    "        if word.isalnum()==0:                             # adds files               \n",
    "            pun+=1\n",
    "            \n",
    "        for expletive in expletives_list:                    # uses regex to find number of times people swear\n",
    "            \n",
    "            if bool(re.search(expletive,word.lower())):\n",
    "                expletives+=1\n",
    "            \n",
    "                \n",
    "        if bool(re.search(regex,word.lower())):           #word tokenize broke apart hashtags so we must look \n",
    "            try:\n",
    "                hash_word=word_list[i+1].lower()                      # one word ahead to find hashtag\n",
    "                if hashtag!=0:\n",
    "                    hashtag='#'+hash_word+' '+hashtag \n",
    "                else:\n",
    "                    hashtag='#'+hash_word\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        i+=1\n",
    "        \n",
    "    ratio = uppers/len(word_list)                             # ratio of uppercase/(num_word)\n",
    "    pun = pun/len(word_list)                                  # ratio of punctuation/(num_word)\n",
    "    hashtags.append(hashtag)\n",
    "    upper_count.append(uppers)\n",
    "    num_expletives.append(expletives)\n",
    "    caps_ratio.append(ratio)\n",
    "    pun_ratio.append(pun)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sents'] = sents                    # sentences in each word\n",
    "tweets['words'] = words                    # words in each tweets, NOT PROCESSED \n",
    "tweets['upper_count'] = upper_count        # number of words that are uppercase\n",
    "tweets['caps_ratio'] = caps_ratio          # ratio of words that are uppercase\n",
    "tweets['num_expletives'] = num_expletives  # number of expletives\n",
    "tweets['pun_ratio'] = pun_ratio            # ratio of words that are punctuation \n",
    "tweets['clean_words'] = processed_words     # words in lowercase and after lemmanization \n",
    "tweets['hashtags'] = hashtags                # hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bag of words matrix for each tweet using processed text\n",
    "matrix = CountVectorizer(max_features=2500)\n",
    "X = matrix.fit_transform(tweets['full_text_processed']).toarray()\n",
    "\n",
    "#making dataframe for bag of words and merging \n",
    "X = pd.DataFrame(X)\n",
    "X.columns = matrix.get_feature_names() # \n",
    "\n",
    "X.reset_index(drop=True,inplace=True)\n",
    "tweets.reset_index(drop=True,inplace=True)\n",
    "\n",
    "tweets = pd.concat([tweets,X],axis=1)\n",
    "\n",
    "#creating bag of words for hashtags\n",
    "matrix = CountVectorizer(max_features=2500)\n",
    "Y = matrix.fit_transform(tweets['hashtags']).toarray()\n",
    "hash_list = matrix.get_feature_names()\n",
    "hash_list = ['#'+hashtag for hashtag in hash_list]\n",
    "Y = pd.DataFrame(Y)\n",
    "Y.columns = hash_list\n",
    "Y.reset_index(drop=True,inplace=True)\n",
    "\n",
    "tweets = pd.concat([tweets,Y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = tweets[0:14640]\n",
    "api_data = tweets[14641:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled_data.to_csv(\"labeled_data.csv\",sep=\",\",encoding='utf-8')\n",
    "#api_data.to_csv(\"api_data.csv\",sep=\",\",encoding='utf-8')\n",
    "#tweets.to_csv(\"tweets_processed.csv\",sep=\",\",encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
